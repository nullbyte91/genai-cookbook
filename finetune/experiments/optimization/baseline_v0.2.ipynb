{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Hugging Face fine-tuning without Unsloth\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load model\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  \n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, let's examine the dataset structure\n",
    "print(\"Dataset example structure:\", dataset[0])\n",
    "\n",
    "# Now let's fix the formatting function\n",
    "def format_instruction(example):\n",
    "    # Process a single example (batched=False)\n",
    "    formatted_texts = []\n",
    "    conversations = example[\"conversations\"]\n",
    "    \n",
    "    # Check if conversations is a list of dictionaries\n",
    "    if isinstance(conversations, list) and all(isinstance(x, dict) for x in conversations):\n",
    "        for i in range(0, len(conversations), 2):\n",
    "            if i + 1 < len(conversations):\n",
    "                instruction = conversations[i][\"value\"]\n",
    "                output = conversations[i + 1][\"value\"]\n",
    "                \n",
    "                # Format as instruction example\n",
    "                formatted_text = f\"<s>[INST] {instruction} [/INST] {output}</s>\"\n",
    "                formatted_texts.append(formatted_text)\n",
    "    \n",
    "    return {\"formatted_text\": formatted_texts}\n",
    "\n",
    "print(\"Formatting conversations...\")\n",
    "# Use batched=False to process one example at a time\n",
    "formatted_dataset = dataset.map(\n",
    "    format_instruction,\n",
    "    batched=False,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Verify the formatting worked\n",
    "print(\"Checking first formatted example:\")\n",
    "if len(formatted_dataset) > 0 and len(formatted_dataset[0][\"formatted_text\"]) > 0:\n",
    "    print(formatted_dataset[0][\"formatted_text\"][0][:100] + \"...\")  # Print first 100 chars\n",
    "\n",
    "# Continue with tokenization\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize_function(examples):\n",
    "    # Flatten the list of texts since each example now contains a list of formatted texts\n",
    "    texts_to_tokenize = [text for text_list in examples[\"formatted_text\"] for text in text_list]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts_to_tokenize,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create input_ids and labels (for causal language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"formatted_text\"]\n",
    ")\n",
    "\n",
    "print(f\"Dataset size after processing: {len(tokenized_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "print(\"Splitting dataset...\")\n",
    "train_val_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "eval_dataset = train_val_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not using masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Configure training arguments\n",
    "print(\"Setting up training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama3_full_finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile=False,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=32,  # Increase for memory efficiency\n",
    "    max_steps=30,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-5,  # Lower learning rate for stability\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    # Use fp16 if bfloat16 not available\n",
    "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    # Add gradient clipping\n",
    "    max_grad_norm=1.0,\n",
    "    # Disable tqdm progress bar if running in notebook\n",
    "    disable_tqdm=False,\n",
    "    # Report to console only\n",
    "    report_to=\"none\",\n",
    "    # Load best model at end of training\n",
    "    load_best_model_at_end=True,\n",
    "    # Optimizer\n",
    "    optim=\"adamw_torch\",\n",
    "    # Save total limit (save disk space)\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Step 4: Create the Trainer\n",
    "print(\"Setting up trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Start training\n",
    "print(\"Starting training...\")\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "    # Step 6: Save the model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(\"llama3_finetuned_full\")\n",
    "    \n",
    "    # Run final evaluation\n",
    "    print(\"Running final evaluation...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Final perplexity: {math.exp(eval_results['eval_loss'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    \n",
    "    # Print more diagnostic information\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try to save the model even if training failed\n",
    "    try:\n",
    "        print(\"Attempting to save partial model...\")\n",
    "        trainer.save_model(\"llama3_finetuned_partial\")\n",
    "    except:\n",
    "        print(\"Could not save partial model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
