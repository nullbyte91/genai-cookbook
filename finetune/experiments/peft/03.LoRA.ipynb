{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nullbyte/Desktop/my_git/genai-cookbook/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-20 18:11:51.799126: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745152911.810684  183252 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745152911.814227  183252 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745152911.823348  183252 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745152911.823359  183252 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745152911.823360  183252 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745152911.823361  183252 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-20 18:11:51.826194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-20 18:11:53,299] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ------------------ Custom Callback for Tracking Metrics ------------------ #\n",
    "class MetricsTrackingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to track and save training metrics during training.\n",
    "    Includes loss, validation loss, and training time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir=\"./logs\"):\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.train_times = []\n",
    "        self.start_time = None\n",
    "        \n",
    "        # Setup logging\n",
    "        self.log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "        logging.basicConfig(\n",
    "            filename=self.log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Record the starting time when training begins.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.logger.info(f\"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(f\"Training arguments: {args}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Record metrics at each logging step.\"\"\"\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Record training loss\n",
    "        if 'loss' in logs:\n",
    "            self.train_losses.append((state.global_step, logs['loss']))\n",
    "            self.logger.info(f\"Step {state.global_step}: Training loss = {logs['loss']}\")\n",
    "        \n",
    "        # Record eval loss\n",
    "        if 'eval_loss' in logs:\n",
    "            self.eval_losses.append((state.global_step, logs['eval_loss']))\n",
    "            self.logger.info(f\"Step {state.global_step}: Evaluation loss = {logs['eval_loss']}\")\n",
    "        \n",
    "        # Record elapsed time\n",
    "        if self.start_time is not None:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            self.train_times.append((state.global_step, elapsed_time))\n",
    "            self.logger.info(f\"Step {state.global_step}: Training time = {elapsed_time:.2f}s\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Save all metrics at the end of training.\"\"\"\n",
    "        # Calculate total training time\n",
    "        total_time = time.time() - self.start_time if self.start_time is not None else 0\n",
    "        \n",
    "        # Log final metrics\n",
    "        self.logger.info(f\"Training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(f\"Total training time: {total_time:.2f} seconds\")\n",
    "        self.logger.info(f\"Final training loss: {self.train_losses[-1][1] if self.train_losses else 'N/A'}\")\n",
    "        self.logger.info(f\"Final evaluation loss: {self.eval_losses[-1][1] if self.eval_losses else 'N/A'}\")\n",
    "        \n",
    "        # Save metrics to JSON file\n",
    "        metrics_file = os.path.join(self.log_dir, \"training_metrics.json\")\n",
    "        metrics = {\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"eval_losses\": self.eval_losses,\n",
    "            \"train_times\": self.train_times,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        # Create visualization of training progress\n",
    "        self.visualize_training_progress()\n",
    "    \n",
    "    def visualize_training_progress(self):\n",
    "        \"\"\"Create visualization of training and validation loss over time.\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot training loss\n",
    "        if self.train_losses:\n",
    "            steps, losses = zip(*self.train_losses)\n",
    "            plt.plot(steps, losses, label='Training Loss')\n",
    "        \n",
    "        # Plot validation loss\n",
    "        if self.eval_losses:\n",
    "            steps, losses = zip(*self.eval_losses)\n",
    "            plt.plot(steps, losses, label='Validation Loss')\n",
    "        \n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(self.log_dir, \"training_loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ------------------ Dataset Preparation ------------------ #\n",
    "class CodeAlpacaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for CodeAlpaca data with Python code examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=512, data_subset=\"train\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Tokenizer to use for encoding\n",
    "            max_length: Maximum sequence length\n",
    "            data_subset: Data subset to use (\"train\" or \"validation\")\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load dataset\n",
    "        self.alpaca_data = load_dataset(\"HuggingFaceH4/CodeAlpaca_20K\", split=\"train\")\n",
    "        \n",
    "        # Filter for Python code\n",
    "        python_keywords = ['def ', 'import ', 'lambda ', 'class ']\n",
    "        def is_python_code(text):\n",
    "            return any(keyword in text for keyword in python_keywords)\n",
    "        \n",
    "        self.python_dataset = self.alpaca_data.filter(lambda example: is_python_code(example['completion']))\n",
    "        print(f\"Loaded {len(self.python_dataset)} Python code examples from CodeAlpaca dataset\")\n",
    "        \n",
    "        # Split into train/validation sets (90% / 10%)\n",
    "        if data_subset == \"train\":\n",
    "            self.dataset = self.python_dataset.select(range(int(len(self.python_dataset) * 0.9)))\n",
    "        else:  # validation\n",
    "            self.dataset = self.python_dataset.select(range(int(len(self.python_dataset) * 0.9), len(self.python_dataset)))\n",
    "        \n",
    "        print(f\"Using {len(self.dataset)} examples for {data_subset}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a formatted and tokenized example.\"\"\"\n",
    "        example = self.dataset[idx]\n",
    "        \n",
    "        # Format the input\n",
    "        input_text = self.format_example(example)\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        item = {key: val.squeeze(0) for key, val in encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def format_example(self, example):\n",
    "        \"\"\"Format an example for instruction fine-tuning.\"\"\"\n",
    "        return (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['completion']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from gpt2-medium...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from gpt2-medium...\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup model\n",
    "print(f\"Loading model from {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "\n",
    "# Configure LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,                     # Rank of the update matrices\n",
    "    lora_alpha=32,           # LoRA scaling factor\n",
    "    lora_dropout=0.1,        # Dropout probability for LoRA layers\n",
    "    # Target modules to apply LoRA to (specific to GPT-2 architecture)\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nullbyte/Desktop/my_git/genai-cookbook/.venv/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/nullbyte/Desktop/my_git/genai-cookbook/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/nullbyte/Desktop/my_git/genai-cookbook/.venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the LoRA model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 361,114,624 || trainable%: 1.7422\n"
     ]
    }
   ],
   "source": [
    "# Print trainable parameters information\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6409 Python code examples from CodeAlpaca dataset\n",
      "Using 5768 examples for train\n",
      "Loaded 6409 Python code examples from CodeAlpaca dataset\n",
      "Using 641 examples for validation\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CodeAlpacaDataset(tokenizer, max_length=max_length, data_subset=\"train\")\n",
    "eval_dataset = CodeAlpacaDataset(tokenizer, max_length=max_length, data_subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "tokenizer=tokenizer,\n",
    "mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./outputs\"\n",
    "batch_size = 2\n",
    "num_train_epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "eval_steps = 500\n",
    "save_steps = 1000\n",
    "logging_steps = 100\n",
    "learning_rate = 5e-4  # Higher learning rate for adapters\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 500\n",
    "early_stopping_patience = 3\n",
    "per_device_train_batch_size = 2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"checkpoints\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_steps=250,\n",
    "    save_steps=250,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    MetricsTrackingCallback(log_dir=os.path.join(output_dir, \"logs\")),\n",
    "    EarlyStoppingCallback(early_stopping_patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting adapter fine-tuning for 3 epochs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 39:22, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.915187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.879800</td>\n",
       "      <td>0.812423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.827600</td>\n",
       "      <td>0.760824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.764600</td>\n",
       "      <td>0.739933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.9716171229327166, metrics={'train_runtime': 2364.958, 'train_samples_per_second': 7.317, 'train_steps_per_second': 0.457, 'total_flos': 1.636675620962304e+16, 'train_loss': 0.9716171229327166, 'epoch': 2.9930651872399445})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Starting adapter fine-tuning for {num_train_epochs} epochs...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./outputs/gpt2-medium_lora_eps\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = os.path.join(output_dir, \"gpt2-medium_lora_eps\")\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./outputs/gpt2-medium_lora_python/tokenizer_config.json',\n",
       " './outputs/gpt2-medium_lora_python/special_tokens_map.json',\n",
       " './outputs/gpt2-medium_lora_python/vocab.json',\n",
       " './outputs/gpt2-medium_lora_python/merges.txt',\n",
       " './outputs/gpt2-medium_lora_python/added_tokens.json',\n",
       " './outputs/gpt2-medium_lora_python/tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training\n",
    "peft_model_path = os.path.join(output_dir, \"gpt2-medium_lora_python\")\n",
    "model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nullbyte/Desktop/my_git/genai-cookbook/.venv/lib/python3.10/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.12.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.12.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.12.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.12.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.12.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.12.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.12.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.12.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.13.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.13.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.13.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.13.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.13.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.13.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.13.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.13.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.14.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.14.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.14.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.14.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.14.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.14.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.14.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.14.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.15.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.15.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.15.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.15.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.15.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.15.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.15.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.15.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.16.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.16.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.16.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.16.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.16.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.16.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.16.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.16.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.17.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.17.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.17.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.17.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.17.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.17.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.17.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.17.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.18.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.18.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.18.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.18.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.18.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.18.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.18.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.18.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.19.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.19.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.19.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.19.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.19.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.19.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.19.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.19.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.20.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.20.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.20.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.20.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.20.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.20.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.20.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.20.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.21.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.21.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.21.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.21.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.21.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.21.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.21.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.21.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.22.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.22.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.22.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.22.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.22.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.22.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.22.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.22.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.23.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.23.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.23.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.23.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.23.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.23.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.23.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.23.mlp.c_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "# When loading later:\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# First load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Then load the PEFT adapters\n",
    "model_path = \"./outputs/gpt2-medium_lora_eps\"\n",
    "loaded_model = PeftModel.from_pretrained(base_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nullbyte/Desktop/my_git/genai-cookbook/.venv/lib/python3.10/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.12.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.12.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.12.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.12.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.12.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.12.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.12.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.12.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.13.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.13.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.13.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.13.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.13.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.13.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.13.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.13.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.14.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.14.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.14.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.14.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.14.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.14.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.14.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.14.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.15.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.15.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.15.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.15.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.15.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.15.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.15.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.15.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.16.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.16.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.16.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.16.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.16.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.16.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.16.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.16.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.17.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.17.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.17.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.17.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.17.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.17.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.17.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.17.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.18.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.18.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.18.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.18.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.18.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.18.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.18.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.18.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.19.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.19.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.19.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.19.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.19.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.19.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.19.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.19.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.20.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.20.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.20.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.20.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.20.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.20.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.20.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.20.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.21.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.21.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.21.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.21.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.21.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.21.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.21.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.21.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.22.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.22.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.22.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.22.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.22.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.22.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.22.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.22.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.23.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.23.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.23.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.23.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.23.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.23.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.23.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.23.mlp.c_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 1024)\n",
       "        (wpe): Embedding(1024, 1024)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=3072, nx=1024)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=1024, nx=1024)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=4096, nx=1024)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=1024, nx=4096)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference \n",
    "# Path to your saved LoRA adapters\n",
    "lora_model_path = \"./outputs/gpt2-medium_lora_python\"  # Adjust path as needed\n",
    "\n",
    "# Step 1: Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Step 2: Load the LoRA adapters onto the base model\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "\n",
    "# Step 3: Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model = lora_model.to(device)\n",
    "lora_model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_python_benchmark import CodeBLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Alpaca-style Fine-tuning ------------------ #\n",
    "def prepare_alpaca_dataset():\n",
    "    \"\"\"\n",
    "    Load and filter the CodeAlpaca dataset for Python code examples.\n",
    "    \n",
    "    Returns:\n",
    "        Filtered dataset with Python code examples\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    alpaca_data = load_dataset(\"HuggingFaceH4/CodeAlpaca_20K\", split=\"train\")\n",
    "    \n",
    "    # Filter for Python code\n",
    "    python_keywords = ['def ', 'import ', 'lambda ']\n",
    "    def is_python_code(text):\n",
    "        return any(keyword in text for keyword in python_keywords)\n",
    "    \n",
    "    python_dataset = alpaca_data.filter(lambda example: is_python_code(example['completion']))\n",
    "    return python_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(prompt, max_length=500, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate Python code given a natural language prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Natural language description of the code to generate\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Sampling temperature\n",
    "        \n",
    "    Returns:\n",
    "        Generated code as a string\n",
    "    \"\"\"\n",
    "    # Format the prompt for code generation\n",
    "    formatted_prompt = f\"# Python program to {prompt}\\n\\ndef\"\n",
    "    \n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate code\n",
    "    output = lora_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated code\n",
    "    generated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the code part (remove the prompt)\n",
    "    code = generated_code[len(formatted_prompt):]\n",
    "    \n",
    "    # Format the code for better presentation\n",
    "    formatted_code = f\"def{code}\"\n",
    "    \n",
    "    return formatted_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 benchmark questions from benchmark_data.json\n",
      "Loaded 20 benchmark questions from benchmark_data.json\n"
     ]
    }
   ],
   "source": [
    "from data_loader import *\n",
    "data_loader = BenchmarkDataManager(\"benchmark_data.json\")\n",
    "data_loader.load_data()\n",
    "data = data_loader.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: find the maximum element in a list\n",
      "reference: def find_max(lst):\n",
      "    if not lst:\n",
      "        return None\n",
      "    max_val = lst[0]\n",
      "    for val in lst:\n",
      "        if val > max_val:\n",
      "            max_val = val\n",
      "    return max_val\n",
      "generated_code: def max_element ( list ):\n",
      "\n",
      "\"\"\" Returns the maximum element in a list \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_max ( list ):\n",
      "\n",
      "\"\"\" Returns the maximum element in a list \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_first ( list ):\n",
      "\n",
      "\"\"\" Returns the first element in a list \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_last ( list ):\n",
      "\n",
      "\"\"\" Returns the last element in a list \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_first_last ( list ):\n",
      "\n",
      "\"\"\" Returns the first element in a list last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the last element in a list last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_first_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the first element in a list first last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_first_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the first element in a list first last last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the last element in a list last last last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_last_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the last element in a list last last last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_first_last_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the first element in a list first last last last last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_last_last_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the last element in a list last last last last last last last last \"\"\"\n",
      "\n",
      "return max (list, 1 )\n",
      "\n",
      "def find_last_last_last_last ( list ):\n",
      "\n",
      "\"\"\" Returns the last element in a list last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last last\n",
      "scores: {'bleu': np.float64(0.0), 'syntax': 0, 'dataflow': 0.0, 'keyword': 0.42857142857142855, 'codebleu': np.float64(0.10714285714285714)}\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "evaluator = CodeBLEU()\n",
    "for item in data:\n",
    "    question = item[\"question\"]\n",
    "    reference = item[\"reference\"]\n",
    "    generated_code = generate_code(question)\n",
    "    scores = evaluator.calculate_codebleu(reference, generated_code)\n",
    "    print(\"Question: {}\".format(question))\n",
    "    print(\"reference: {}\".format(reference))\n",
    "    print(\"generated_code: {}\".format(generated_code))\n",
    "    print(\"scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
