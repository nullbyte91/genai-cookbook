{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ------------------ Custom Callback for Tracking Metrics ------------------ #\n",
    "class MetricsTrackingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to track and save training metrics during training.\n",
    "    Includes loss, validation loss, and training time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir=\"./logs\"):\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.train_times = []\n",
    "        self.start_time = None\n",
    "        \n",
    "        # Setup logging\n",
    "        self.log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "        logging.basicConfig(\n",
    "            filename=self.log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Record the starting time when training begins.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.logger.info(f\"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(f\"Training arguments: {args}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Record metrics at each logging step.\"\"\"\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Record training loss\n",
    "        if 'loss' in logs:\n",
    "            self.train_losses.append((state.global_step, logs['loss']))\n",
    "            self.logger.info(f\"Step {state.global_step}: Training loss = {logs['loss']}\")\n",
    "        \n",
    "        # Record eval loss\n",
    "        if 'eval_loss' in logs:\n",
    "            self.eval_losses.append((state.global_step, logs['eval_loss']))\n",
    "            self.logger.info(f\"Step {state.global_step}: Evaluation loss = {logs['eval_loss']}\")\n",
    "        \n",
    "        # Record elapsed time\n",
    "        if self.start_time is not None:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            self.train_times.append((state.global_step, elapsed_time))\n",
    "            self.logger.info(f\"Step {state.global_step}: Training time = {elapsed_time:.2f}s\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Save all metrics at the end of training.\"\"\"\n",
    "        # Calculate total training time\n",
    "        total_time = time.time() - self.start_time if self.start_time is not None else 0\n",
    "        \n",
    "        # Log final metrics\n",
    "        self.logger.info(f\"Training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(f\"Total training time: {total_time:.2f} seconds\")\n",
    "        self.logger.info(f\"Final training loss: {self.train_losses[-1][1] if self.train_losses else 'N/A'}\")\n",
    "        self.logger.info(f\"Final evaluation loss: {self.eval_losses[-1][1] if self.eval_losses else 'N/A'}\")\n",
    "        \n",
    "        # Save metrics to JSON file\n",
    "        metrics_file = os.path.join(self.log_dir, \"training_metrics.json\")\n",
    "        metrics = {\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"eval_losses\": self.eval_losses,\n",
    "            \"train_times\": self.train_times,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        # Create visualization of training progress\n",
    "        self.visualize_training_progress()\n",
    "    \n",
    "    def visualize_training_progress(self):\n",
    "        \"\"\"Create visualization of training and validation loss over time.\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot training loss\n",
    "        if self.train_losses:\n",
    "            steps, losses = zip(*self.train_losses)\n",
    "            plt.plot(steps, losses, label='Training Loss')\n",
    "        \n",
    "        # Plot validation loss\n",
    "        if self.eval_losses:\n",
    "            steps, losses = zip(*self.eval_losses)\n",
    "            plt.plot(steps, losses, label='Validation Loss')\n",
    "        \n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(self.log_dir, \"training_loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ------------------ Dataset Preparation ------------------ #\n",
    "class CodeAlpacaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for CodeAlpaca data with Python code examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=512, data_subset=\"train\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Tokenizer to use for encoding\n",
    "            max_length: Maximum sequence length\n",
    "            data_subset: Data subset to use (\"train\" or \"validation\")\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load dataset\n",
    "        self.alpaca_data = load_dataset(\"HuggingFaceH4/CodeAlpaca_20K\", split=\"train\")\n",
    "        \n",
    "        # Filter for Python code\n",
    "        python_keywords = ['def ', 'import ', 'lambda ', 'class ']\n",
    "        def is_python_code(text):\n",
    "            return any(keyword in text for keyword in python_keywords)\n",
    "        \n",
    "        self.python_dataset = self.alpaca_data.filter(lambda example: is_python_code(example['completion']))\n",
    "        print(f\"Loaded {len(self.python_dataset)} Python code examples from CodeAlpaca dataset\")\n",
    "        \n",
    "        # Split into train/validation sets (90% / 10%)\n",
    "        if data_subset == \"train\":\n",
    "            self.dataset = self.python_dataset.select(range(int(len(self.python_dataset) * 0.9)))\n",
    "        else:  # validation\n",
    "            self.dataset = self.python_dataset.select(range(int(len(self.python_dataset) * 0.9), len(self.python_dataset)))\n",
    "        \n",
    "        print(f\"Using {len(self.dataset)} examples for {data_subset}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a formatted and tokenized example.\"\"\"\n",
    "        example = self.dataset[idx]\n",
    "        \n",
    "        # Format the input\n",
    "        input_text = self.format_example(example)\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        item = {key: val.squeeze(0) for key, val in encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def format_example(self, example):\n",
    "        \"\"\"Format an example for instruction fine-tuning.\"\"\"\n",
    "        return (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['completion']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from gpt2-medium...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from gpt2-medium...\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup model\n",
    "print(f\"Loading model from {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of an adapter layer for transformer models.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, adapter_dim, dropout_rate=0.1):\n",
    "        super(AdapterLayer, self).__init__()\n",
    "        \n",
    "        # Down-projection\n",
    "        self.down_proj = torch.nn.Linear(input_dim, adapter_dim)\n",
    "        \n",
    "        # Non-linearity (GELU)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        \n",
    "        # Up-projection\n",
    "        self.up_proj = torch.nn.Linear(adapter_dim, input_dim)\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.layer_norm = torch.nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "        # Make sure parameters require gradients\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for stability.\"\"\"\n",
    "        # Initialize down projection with small values\n",
    "        torch.nn.init.normal_(self.down_proj.weight, std=1e-3)\n",
    "        torch.nn.init.zeros_(self.down_proj.bias)\n",
    "        \n",
    "        # Initialize up projection with zeros for residual stability\n",
    "        torch.nn.init.zeros_(self.up_proj.weight)\n",
    "        torch.nn.init.zeros_(self.up_proj.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with residual connection.\"\"\"\n",
    "        # Save residual\n",
    "        residual = x\n",
    "        \n",
    "        # Ensure x is on the same device as our parameters\n",
    "        device = next(self.parameters()).device\n",
    "        if x.device != device:\n",
    "            x = x.to(device)\n",
    "            residual = residual.to(device)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Down-projection\n",
    "        x = self.down_proj(x)\n",
    "        \n",
    "        # Activation\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Up-projection\n",
    "        x = self.up_proj(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        return residual + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_adapters_to_model(model, adapter_dim):\n",
    "    \"\"\"\n",
    "    Add adapter layers to a GPT-2 model properly with correct device placement.\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT-2 model\n",
    "        adapter_dim: Dimension of the adapter bottleneck\n",
    "    \n",
    "    Returns:\n",
    "        Modified model with adapters\n",
    "    \"\"\"\n",
    "    # Determine the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Model is on device: {device}\")\n",
    "    \n",
    "    # Freeze all parameters in the original model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Get the hidden size from the model config\n",
    "    hidden_size = model.config.hidden_size\n",
    "    \n",
    "    # Create a container to hold our adapters so they remain in memory\n",
    "    if not hasattr(model, 'adapters'):\n",
    "        model.adapters = {}\n",
    "    \n",
    "    # Add adapters to each transformer block\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        # Create and attach the adapters to the model to ensure they're tracked\n",
    "        attn_adapter_name = f\"adapter_attn_{i}\"\n",
    "        mlp_adapter_name = f\"adapter_mlp_{i}\"\n",
    "        \n",
    "        # Create adapters and make sure their parameters require gradients\n",
    "        attn_adapter = AdapterLayer(hidden_size, adapter_dim).to(device)\n",
    "        mlp_adapter = AdapterLayer(hidden_size, adapter_dim).to(device)\n",
    "        \n",
    "        # Store adapters in the model\n",
    "        model.adapters[attn_adapter_name] = attn_adapter\n",
    "        model.adapters[mlp_adapter_name] = mlp_adapter\n",
    "        \n",
    "        # Create closures that correctly capture the adapters\n",
    "        def make_attn_forward(orig_forward, adapter):\n",
    "            def new_forward(self, *args, **kwargs):\n",
    "                output = orig_forward(*args, **kwargs)\n",
    "                # Check if the output is on the same device as the adapter\n",
    "                if output[0].device != next(adapter.parameters()).device:\n",
    "                    print(f\"Warning: Device mismatch - output: {output[0].device}, adapter: {next(adapter.parameters()).device}\")\n",
    "                \n",
    "                # Apply adapter to the output\n",
    "                modified_output = adapter(output[0])\n",
    "                # Return as a tuple like the original output\n",
    "                return (modified_output,) + output[1:]\n",
    "            return new_forward\n",
    "        \n",
    "        def make_mlp_forward(orig_forward, adapter):\n",
    "            def new_forward(self, x):\n",
    "                output = orig_forward(x)\n",
    "                # Check if the output is on the same device as the adapter\n",
    "                if output.device != next(adapter.parameters()).device:\n",
    "                    print(f\"Warning: Device mismatch - output: {output.device}, adapter: {next(adapter.parameters()).device}\")\n",
    "                \n",
    "                # Apply adapter to the output\n",
    "                return adapter(output)\n",
    "            return new_forward\n",
    "        \n",
    "        # Store original forward methods\n",
    "        original_attn_forward = block.attn.forward\n",
    "        original_mlp_forward = block.mlp.forward\n",
    "        \n",
    "        # Apply the new forward methods with proper closure scope\n",
    "        block.attn.forward = make_attn_forward(\n",
    "            original_attn_forward, \n",
    "            model.adapters[attn_adapter_name]\n",
    "        ).__get__(block.attn)\n",
    "        \n",
    "        block.mlp.forward = make_mlp_forward(\n",
    "            original_mlp_forward, \n",
    "            model.adapters[mlp_adapter_name]\n",
    "        ).__get__(block.mlp)\n",
    "    \n",
    "    # Register adapters as proper modules to ensure they're tracked\n",
    "    for name, adapter in model.adapters.items():\n",
    "        model.add_module(name, adapter)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/all_params:.2%} of total)\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n",
      "Trainable parameters: 6,441,984 (1.78% of total)\n"
     ]
    }
   ],
   "source": [
    "# 1. Custom adapters\n",
    "adapter_dim = 64  # Bottleneck dimension, typically 1/8 to 1/64 of hidden size\n",
    "model = add_adapters_to_model(model, adapter_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6409 Python code examples from CodeAlpaca dataset\n",
      "Using 5768 examples for train\n",
      "Loaded 6409 Python code examples from CodeAlpaca dataset\n",
      "Using 641 examples for validation\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CodeAlpacaDataset(tokenizer, max_length=max_length, data_subset=\"train\")\n",
    "eval_dataset = CodeAlpacaDataset(tokenizer, max_length=max_length, data_subset=\"validation\")\n",
    "\n",
    "\n",
    "# Setup data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets and move tensors to the correct device in the training loop\n",
    "# def collate_fn(batch):\n",
    "#     # Default collation\n",
    "#     collated = data_collator(batch)\n",
    "#     # Move to device\n",
    "#     return {k: v.to(device) for k, v in collated.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 2\n",
    "# Setup data loader with the custom collate function\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=per_device_train_batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./outputs\"\n",
    "batch_size = 1\n",
    "num_train_epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "eval_steps = 500\n",
    "save_steps = 1000\n",
    "logging_steps = 100\n",
    "learning_rate = 5e-4  # Higher learning rate for adapters\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 500\n",
    "early_stopping_patience = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"checkpoints\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_steps=250,\n",
    "    save_steps=250,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    MetricsTrackingCallback(log_dir=os.path.join(output_dir, \"logs\")),\n",
    "    EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting adapter fine-tuning for 3 epochs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 33:14, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.113700</td>\n",
       "      <td>0.935739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>0.829503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.781986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.787600</td>\n",
       "      <td>0.754700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=1.0035847416630497, metrics={'train_runtime': 1996.0074, 'train_samples_per_second': 8.669, 'train_steps_per_second': 0.541, 'total_flos': 1.6374739463307264e+16, 'train_loss': 1.0035847416630497, 'epoch': 2.9930651872399445})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Starting adapter fine-tuning for {num_train_epochs} epochs...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./outputs/gpt2-medium_adapter_eps\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = os.path.join(output_dir, \"gpt2-medium_adapter_eps\")\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
