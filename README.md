# genai-cookbook
## 📌 Table of Contents
1. [Introduction](#-introduction)
2. [🧭 Strategy & Scoping](#-strategy--scoping)
3. [🧱 Data Foundation](#-data-foundation)
4. [🔁 Iterative Development & Evaluation](#-iterative-development--evaluation)
   - [3.1 Foundation Model Selection & Baseline Evaluation](#31-foundation-model-selection--baseline-evaluation)
   - [3.2 Application Prototyping](#32-application-prototyping)
   - [3.3 Prompt Engineering & Optimization](#33-prompt-engineering--optimization)
   - [3.4 RAG System Development & Tuning](#34-rag-system-development--tuning)
   - [3.5 Fine-tuning](#35-fine-tuning)
     - [3.5.1 Dataset Preparation](#351-dataset-preparation)
     - [3.5.2 Training Setup](#352-training-setup)
     - [3.5.3 Model Initialisation](#353-model-initialisation)
     - [3.5.4 Selection of Fine-Tuning Technique](#354-selection-of-fine-tuning-technique)
   - [3.6 Comprehensive Evaluation](#36-comprehensive-evaluation)
5. [🚀 Productionization & Operations (LLMOps)](#-productionization--operations-llmops)
6. [🔍♻️ Continuous Monitoring & Improvement](#-continuous-monitoring--improvement)
7. [📚 Appendix](#-appendix-foundational-concepts)
7. [🤝 Contributing](#-contributing)
8. [📜 License](#-license)

## 📘 Introduction
This repository is a research-driven framework for adopting and fine-tuning Large Language Models (LLMs) effectively and responsibly.

Instead of jumping directly into fine-tuning, this project explores a <b>layered approach to LLM system design</b>, combining industry best practices, cost-effective techniques, and deep evaluations. It aims to help you decide <b>when to use prompt engineering, retrieval-augmented generation (RAG), or full/parameter-efficient fine-tuning</b>, based on your task, domain, and constraints.

The goal is to build a modular, extensible pipeline that supports experimentation, benchmarking, and eventual deployment of high-performing, domain-specific LLMs — while maintaining reproducibility, traceability, and practical relevance.


## 🧭 Strategy & Scoping
*(Coming Soon)*

## 🧱 Data Foundation
*(Coming Soon)*

## 🔁 Iterative Development & Evaluation

### 3.1 Foundation Model Selection & Baseline Evaluation
### 3.2 Application Prototyping
### 3.3 Prompt Engineering & Optimization
### 3.4 RAG System Development & Tuning
### 3.5 Fine-Tuning
#### 3.5.1 Dataset Preparation

##### 3.5.1.1 Data collection
#### 3.5.2 Training Setup
#### 3.5.3 Model Initialisation
#### 3.5.4 Selection of Fine-Tuning Technique
#### 3.5.5 Practical Examples
1. [Fine-Tuning GPT-2 for AG News Classification](./finetune/experiments/gpt2_ag_news_classifier/README.md)
2. [Fine-Tuning GPT-2 for code generation](./finetune/experiments/gpt2_py_code_generation/README.md)

### 3.6 Comprehensive Evaluation

## 🚀 Productionization & Operations (LLMOps)
*(Coming Soon)*

## 🔍♻️ Continuous Monitoring & Improvement
*(Coming Soon)*

## 📚 Appendix
### Archiecture
1. [The Transformer Architecture](./appendix/transformer_archiecture_overview.md)

## Reference
### Finetune
1. [Data Preparation Guide by Unsloth](https://docs.unsloth.ai/basics/datasets-guide)
2. [LLMDataHub](https://github.com/Zjh-819/LLMDataHub)

### General
1. [awesome-llms-fine-tuning repo 1](https://github.com/Curated-Awesome-Lists/awesome-llms-fine-tuning)
2. [awesome-llms-fine-tuning repo 2](https://github.com/pdaicode/awesome-LLMs-finetuning)

## 🤝 Contributing
We welcome contributors! See CONTRIBUTING.md for guidelines.

## 📜 License
This project uses:
Code: [Apache-2.0](LICENSE.txt)
Content: [LICENSE.CC-BY-NC](LICENSE.CC-BY-NC.txt)

